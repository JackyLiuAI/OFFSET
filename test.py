import numpy as np
import torch
from tqdm import tqdm as tqdm
import torch.nn.functional as F


def test(params, model, testset, category):
    model.eval()
    # Support datasets that expose per-category test queries/targets (FashionIQ)
    if hasattr(testset, 'test_queries') and hasattr(testset, 'test_targets'):
        (test_queries, test_targets, name) = (testset.test_queries, testset.test_targets, category)
    elif category == 'dress' and hasattr(testset, 'test_queries_dress'):
        (test_queries, test_targets, name) = (testset.test_queries_dress, testset.test_targets_dress, 'dress')
    elif category == 'shirt' and hasattr(testset, 'test_queries_shirt'):
        (test_queries, test_targets, name) = (testset.test_queries_shirt, testset.test_targets_shirt, 'shirt')
    elif category == 'toptee' and hasattr(testset, 'test_queries_toptee'):
        (test_queries, test_targets, name) = (testset.test_queries_toptee, testset.test_targets_toptee, 'toptee')
    else:
        raise AttributeError(f"Dataset does not provide test queries/targets for category '{category}'")
    # Optionally cap evaluation size for quick debugging
    max_eval_queries = getattr(params, 'max_eval_queries', 0)
    max_eval_targets = getattr(params, 'max_eval_targets', 0)
    if max_eval_queries and max_eval_queries > 0:
        test_queries = test_queries[:max_eval_queries]
    if max_eval_targets and max_eval_targets > 0:
        test_targets = test_targets[:max_eval_targets]
    with torch.no_grad():
        all_queries = []
        all_imgs = []
        if test_queries:
            # compute test query features

            imgs = []
            imgs_seg = []
            mods = []
            for t in tqdm(test_queries, disable=False if params.local_rank == 0 else True):
                imgs += [t['source_img_data']]
                imgs_seg += [t['source_img_data_seg']]
                mods += [t['mod']['str']]
                if len(imgs) >= params.batch_size or t is test_queries[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()

                    f = model.extract_retrieval_compose(imgs, mods, imgs_seg)

                    f = f.data.cpu().numpy()
                    all_queries += [f]
                    imgs = []
                    imgs_seg = []
                    mods = []

            all_queries = np.concatenate(all_queries)

            # compute all image features
            imgs = []
            imgs_seg = []
            logits = []
            for t in tqdm(test_targets, disable=False if params.local_rank == 0 else True):
                imgs += [t['target_img_data']]
                imgs_seg += [t['target_img_data_seg']]
                if len(imgs) >= params.batch_size or t is test_targets[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()
                    imgs = model.extract_retrieval_target(imgs, imgs_seg).data.cpu().numpy()
                    all_imgs += [imgs]
                    imgs = []
                    imgs_seg = []
            all_imgs = np.concatenate(all_imgs)

    # feature normalization
    for i in range(all_queries.shape[0]):
        all_queries[i, :] /= np.linalg.norm(all_queries[i, :])
    for i in range(all_imgs.shape[0]):
        all_imgs[i, :] /= np.linalg.norm(all_imgs[i, :])
    
    
    # match test queries to target images, get nearest neighbors
    sims = all_queries.dot(all_imgs.T)
    
    test_targets_id = []
    for i in test_targets:
        test_targets_id.append(i['target_img_id'])
    
    if name != 'birds':
        for i, t in enumerate(test_queries):
            sims[i, test_targets_id.index(t['source_img_id'])] = -10e10


    nn_result = [np.argsort(-sims[i, :])[:50] for i in range(sims.shape[0])]

    # compute recalls
    out = []
    for k in [1, 10, 50]:
        r = 0.0
        for i, nns in enumerate(nn_result):
            if test_targets_id.index(test_queries[i]['target_img_id']) in nns[:k]:
                r += 1
        r = 100 * r / len(nn_result)
        out += [('{}_r{}'.format(name, k), r)]

    return out


def test_figAll(params, model, testset, category):
    model.eval()
    if category == 'dress':
        (test_queries, test_targets, name) = (testset.test_queries_dress, testset.test_targets_dress, 'dress')
    elif category == 'shirt':
        (test_queries, test_targets, name) = (testset.test_queries_shirt, testset.test_targets_shirt, 'shirt')
    elif category == 'toptee':
        (test_queries, test_targets, name) = (testset.test_queries_toptee, testset.test_targets_toptee, 'toptee')
    max_eval_queries = getattr(params, 'max_eval_queries', 0)
    max_eval_targets = getattr(params, 'max_eval_targets', 0)
    if max_eval_queries and max_eval_queries > 0:
        test_queries = test_queries[:max_eval_queries]
    if max_eval_targets and max_eval_targets > 0:
        test_targets = test_targets[:max_eval_targets]
    # (test_queries, test_targets, name) = (testset.test_queries, testset.test_targets, category)
    with torch.no_grad():
        all_queries = []
        all_imgs = []
        if test_queries:
            # compute test query features

            imgs = []
            imgs_seg = []
            mods = []
            for t in tqdm(test_queries, disable=False if params.local_rank == 0 else True):
                imgs += [t['source_img_data']]
                imgs_seg += [t['source_img_data_seg']]
                mods += [t['mod']['str']]
                if len(imgs) >= params.batch_size or t is test_queries[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()
                    f = model.extract_retrieval_compose(imgs, mods, imgs_seg)
                    f = f.data.cpu().numpy()
                    all_queries += [f]
                    imgs = []
                    imgs_seg = []
                    mods = []

            all_queries = np.concatenate(all_queries)

            # compute all image features
            imgs = []
            imgs_seg = []
            logits = []
            for t in tqdm(test_targets, disable=False if params.local_rank == 0 else True):
                imgs += [t['target_img_data']]
                imgs_seg += [t['target_img_data_seg']]
                if len(imgs) >= params.batch_size or t is test_targets[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()
                    imgs = model.extract_retrieval_target(imgs, imgs_seg).data.cpu().numpy()
                    all_imgs += [imgs]
                    imgs = []
                    imgs_seg = []
            all_imgs = np.concatenate(all_imgs)

    # feature normalization
    for i in range(all_queries.shape[0]):
        all_queries[i, :] /= np.linalg.norm(all_queries[i, :])
    for i in range(all_imgs.shape[0]):
        all_imgs[i, :] /= np.linalg.norm(all_imgs[i, :])
    
    
    # match test queries to target images, get nearest neighbors
    sims = all_queries.dot(all_imgs.T)
    
    test_targets_id = []
    for i in test_targets:
        test_targets_id.append(i['target_img_id'])
    
    if name != 'birds':
        for i, t in enumerate(test_queries):
            sims[i, test_targets_id.index(t['source_img_id'])] = -10e10


    nn_result = [np.argsort(-sims[i, :])[:50] for i in range(sims.shape[0])]

    # compute recalls
    out = []
    for k in [1, 10, 50]:
        r = 0.0
        for i, nns in enumerate(nn_result):
            if test_targets_id.index(test_queries[i]['target_img_id']) in nns[:k]:
                r += 1
        r = 100 * r / len(nn_result)
        out += [('{}_r{}'.format(name, k), r)]

    return out

def test_cirr_valset(params, model, testset):
    
    model.eval()
    test_queries, test_targets = testset.val_queries, testset.val_targets
    max_eval_queries = getattr(params, 'max_eval_queries', 0)
    max_eval_targets = getattr(params, 'max_eval_targets', 0)
    if max_eval_queries and max_eval_queries > 0:
        test_queries = test_queries[:max_eval_queries]
    if max_eval_targets and max_eval_targets > 0:
        test_targets = test_targets[:max_eval_targets]
    with torch.no_grad():
        all_queries = []
        all_imgs = []

        if test_queries:
            # compute test query features
            imgs = []
            imgs_seg = []
            mods = []
            for t in tqdm(test_queries, disable=False if params.local_rank == 0 else True):
                imgs += [t['source_img_data']]
                imgs_seg += [t['source_img_data_seg']]
                mods += [t['mod']['str']]
                if len(imgs) >= params.batch_size or t is test_queries[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()
                    import time
                    # start_time = time.time()
                    f = model.extract_retrieval_compose(imgs, mods, imgs_seg)
                    # end_time = time.time()
                    # elapsed_time = (end_time - start_time) * 1000000
                    # print(f"Model forward pass took {elapsed_time:.6f} microseconds")
                    # print(f"batch {imgs.shape[0]}")
                    f = f.data.cpu().numpy()
                    all_queries += [f]
                    imgs = []
                    imgs_seg = []
                    mods = []
            all_queries = np.concatenate(all_queries)

            # compute all image features
            imgs = []
            imgs_seg = []
            logits = []
            for t in tqdm(test_targets, disable=False if params.local_rank == 0 else True):
                imgs += [t['target_img_data']]
                imgs_seg += [t['target_img_data_seg']]
                if len(imgs) >= params.batch_size or t is test_targets[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()
                    imgs = model.extract_retrieval_target(imgs, imgs_seg).data.cpu().numpy()
                    all_imgs += [imgs]
                    imgs = []
                    imgs_seg = []
            all_imgs = np.concatenate(all_imgs)

    # feature normalization
    for i in range(all_queries.shape[0]):
        all_queries[i, :] /= np.linalg.norm(all_queries[i, :])

    for i in range(all_imgs.shape[0]):
        all_imgs[i, :] /= np.linalg.norm(all_imgs[i, :])
    
    
    # match test queries to target images, get nearest neighbors
    sims = all_queries.dot(all_imgs.T)


    test_targets_id = []
    for i in test_targets:
        test_targets_id.append(i['target_img_id'])
    for i, t in enumerate(test_queries):
        sims[i, test_targets_id.index(t['source_img_id'])] = -10e10


    nn_result = [np.argsort(-sims[i, :]) for i in range(sims.shape[0])] # (m,n)

    # all set recalls
    cirr_out = []
    for k in [1, 5, 10, 50]:
        r = 0.0
        for i, nns in enumerate(nn_result):
            if test_targets_id.index(test_queries[i]['target_img_id']) in nns[:k]:
                r += 1
        r = 100 * r / len(nn_result)
        cirr_out += [('{}_r{}'.format('cirr',k), r)]

    # subset recalls
    for k in [1, 2, 3]:
        r = 0.0
        for i, nns in enumerate(nn_result):

            subset = np.array([test_targets_id.index(idx) for idx in test_queries[i]['subset_id']]) 
            subset_mask = (nns[..., None] == subset[None, ...]).sum(-1).astype(bool) 
            subset_label = nns[subset_mask] 
            if test_targets_id.index(test_queries[i]['target_img_id']) in subset_label[:k]:
                r += 1
        r = 100 * r / len(nn_result)
        cirr_out += [('{}_subset_r{}'.format('cirr', k), r)]

    return cirr_out



def test_fashion200k_dataset(params, model, testset):
    """Tests a model over the given testset."""
    
    model.eval()
    test_queries = testset.get_test_queries()
    with torch.no_grad():
        all_imgs = []
        all_captions = []
        all_queries = []
        all_target_captions = []
        if test_queries:
            # compute test query features
            imgs = []
            imgs_seg = []
            mods = []
            for t in tqdm(test_queries):
                imgs += [testset.get_img(t['source_img_id'])]
                imgs_seg += [testset.get_saved_segImg(t['source_img_id'])]
                mods += [t['mod']['str']]
                if len(imgs) >= params.batch_size or t is test_queries[-1]:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float()
                    imgs = torch.autograd.Variable(imgs).cuda()
                    imgs_seg = torch.stack(imgs_seg).float()
                    imgs_seg = torch.autograd.Variable(imgs_seg).cuda()
                    mods = [t.encode('utf-8').decode('utf-8') for t in mods]
                    f = model.extract_retrieval_compose(imgs, mods, imgs_seg)
                    f = f.data.cpu().numpy()
                    all_queries += [f]
                    imgs = []
                    mods = []
                    imgs_seg = []
            all_queries = np.concatenate(all_queries)
            all_target_captions = [t['target_caption'] for t in test_queries]

            # compute all image features
            imgs = []
            imgs_seg = []
            for i in tqdm(range(len(testset.imgs))):
                imgs += [testset.get_img(i)]
                imgs_seg += [testset.get_saved_segImg(i)]
                if len(imgs) >= params.batch_size or i == len(testset.imgs) - 1:
                    if 'torch' not in str(type(imgs[0])):
                        imgs = [torch.from_numpy(d).float() for d in imgs]
                        imgs_seg = [torch.from_numpy(d).float() for d in imgs_seg]
                    imgs = torch.stack(imgs).float().cuda()
                    imgs_seg = torch.stack(imgs_seg).float().cuda()
                    imgs = model.extract_retrieval_target(imgs, imgs_seg).data.cpu().numpy()
                    all_imgs += [imgs]
                    imgs = []
                    imgs_seg = []
            all_imgs = np.concatenate(all_imgs)
            all_captions = [img['captions'][0] for img in testset.imgs]

        # feature normalization
        for i in range(all_queries.shape[0]):
            all_queries[i, :] /= np.linalg.norm(all_queries[i, :])

        for i in range(all_imgs.shape[0]):
            all_imgs[i, :] /= np.linalg.norm(all_imgs[i, :])

        # match test queries to target images, get nearest neighbors
        sims = all_queries.dot(all_imgs.T)
        if test_queries:
            for i, t in enumerate(test_queries):
                sims[i, t['source_img_id']] = -10e10  
        nn_result = [np.argsort(-sims[i, :])[:110] for i in range(sims.shape[0])]

        # compute recalls
        out = []
        nn_result = [[all_captions[nn] for nn in nns] for nns in nn_result]
        for k in [1, 10, 50]:
            r = 0.0
            for i, nns in enumerate(nn_result):
                if all_target_captions[i] in nns[:k]:
                    r += 1
            r /= len(nn_result)
            out += [('recall_top' + str(k) + '_correct_composition', r)]

        return out
    
